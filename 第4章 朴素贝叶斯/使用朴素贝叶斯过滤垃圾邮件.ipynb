{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明:**\n",
    "\n",
    "将 `email` 文件夹放在当前目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet.union(set(document))\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWord2Vec(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:print('no such word')\n",
    "    return returnVec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算单个类别的词频率\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2\n",
    "    p1Denom = 2\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num/p1Denom)\n",
    "    p0Vect = np.log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类函数\n",
    "\n",
    "def classifyNB(vec,p0Vec,p1Vec,pClass):\n",
    "    p1 = sum(vec*p1Vec)+np.log(pClass)\n",
    "    p0 = sum(vec*p0Vec)+np.log(1-pClass)\n",
    "    return 1 if p1>p0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯词袋模型\n",
    "\n",
    "def bagOfWord2Vec(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件解析\n",
    "\n",
    "def textParse(bigString):\n",
    "    import re \n",
    "    listOfTokens = re.split(r'\\W*',bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok)>2]\n",
    "\n",
    "def spamTest():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#贝叶斯算法实例：过滤垃圾邮件\n",
    "\n",
    "#处理数据长字符串\n",
    "#1 对长字符串进行分割，分隔符为除单词和数字之外的任意符号串\n",
    "#2 将分割后的字符串中所有的大些字母变成小写lower(),并且只\n",
    "#保留单词长度大于3的单词\n",
    "def testParse(bigString):\n",
    "    import re\n",
    "    listOfTokens=re.split(r'\\W',bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok)>2]\n",
    "\n",
    "def spamTest():\n",
    "    #新建三个列表\n",
    "    docList=[];classList=[];fullTest=[]\n",
    "    #i 由1到26\n",
    "    for i in range(1,26):\n",
    "        #打开并读取指定目录下的本文中的长字符串，并进行处理返回\n",
    "        wordList=testParse(open('email/spam/%d.txt' %i).read())\n",
    "        #将得到的字符串列表添加到docList\n",
    "        docList.append(wordList)\n",
    "        #将字符串列表中的元素添加到fullTest\n",
    "        fullTest.extend(wordList)\n",
    "        #类列表添加标签1\n",
    "        classList.append(1)\n",
    "        #打开并取得另外一个类别为0的文件，然后进行处理\n",
    "        wordList=testParse(open('email/ham/%d.txt' %i).read())\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(0)\n",
    "   \n",
    "    #将所有邮件中出现的字符串构建成字符串列表\n",
    "    vocabList=createVocabList(docList)\n",
    "    #构建一个大小为50的整数列表和一个空列表\n",
    "    trainingSet=list(range(50));testSet=[]\n",
    "    #随机选取1~50中的10个数，作为索引，构建测试集\n",
    "    for i in range(10):\n",
    "        #随机选取1~50中的一个整型数\n",
    "        randIndex=int(np.random.uniform(0,len(trainingSet)))\n",
    "        #将选出的数的列表索引值添加到testSet列表中\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        #从整数列表中删除选出的数，防止下次再次选出\n",
    "        #同时将剩下的作为训练集\n",
    "        del(trainingSet[randIndex])\n",
    "    #新建两个列表\n",
    "    trainMat=[];trainClasses=[]\n",
    "    #遍历训练集中的每个字符串列表\n",
    "    for docIndex in trainingSet:\n",
    "        #将字符串列表转为词条向量，然后添加到训练矩阵中\n",
    "        trainMat.append(setOfWord2Vec(vocabList,fullTest[docIndex]))\n",
    "        #将该邮件的类标签存入训练类标签列表中\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    #计算贝叶斯函数需要的概率值并返回\n",
    "    p0V,p1V,pSpam=trainNB0(np.array(trainMat),np.array(trainClasses))\n",
    "    errorCount=0\n",
    "    #遍历测试集中的字符串列表\n",
    "    for docIndex in testSet:\n",
    "        #同样将测试集中的字符串列表转为词条向量\n",
    "        wordVector=setOfWord2Vec(vocabList,docList[docIndex])\n",
    "        #对测试集中字符串向量进行预测分类，分类结果不等于实际结果\n",
    "        if classifyNB(np.array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:\n",
    "            errorCount+=1\n",
    "        print('the error rate is:',float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "no such word\n",
      "the error rate is: 0.0\n",
      "the error rate is: 0.1\n",
      "the error rate is: 0.1\n",
      "the error rate is: 0.1\n",
      "the error rate is: 0.2\n",
      "the error rate is: 0.3\n",
      "the error rate is: 0.3\n",
      "the error rate is: 0.4\n",
      "the error rate is: 0.4\n",
      "the error rate is: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ni\\Anaconda3\\envs\\python3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Ni\\Anaconda3\\envs\\python3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There was a guy at the gas station who told me that if I knew Mandarin\\nand Python I could get a job with the FBI.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('email/ham/%d.txt' %5).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Incredib1e gains in length of 3-4 inches to yourPenis, PERMANANTLY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Incredib1e',\n",
       " 'gains',\n",
       " 'in',\n",
       " 'length',\n",
       " 'of',\n",
       " '3',\n",
       " '4',\n",
       " 'inches',\n",
       " 'to',\n",
       " 'yourPenis',\n",
       " '',\n",
       " 'PERMANANTLY']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\W',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
